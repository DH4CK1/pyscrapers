#!/usr/bin/python3

'''
This script scrapes photos from instagram.com

For instance if you see a profile of a user like this:
    https://www.instagram.com/ada_123456789
then the id for this script will be:
    ada_123456789

References:
- http://docs.python-requests.org/en/master
- http://docs.python-guide.org/en/latest/scenarios/scrape
'''

import requests # for post
import lxml.html # for fromstring
import lxml.etree # for tostring
import json # for loads
import shutil # for copyfileobj
import sys # for argv
import logging # for basicConfig, getLogger
import argparse  # for ArgumentParser
import browser_cookie3 # for firefox
import http.client # for HTTPConnection
import scrape.utils # for download_urls, get_real_content

# code

# register regular expressions with lxml
# this means that we can use regular expression functions like 'match'
# by specifying 're:match' in our xpath expressions
ns = lxml.etree.FunctionNamespace("http://exslt.org/regular-expressions")
ns.prefix = 're'

# set up the logger
logging.basicConfig()
logger=logging.getLogger(__name__)
logger.setLevel(logging.INFO)
#logger.setLevel(logging.DEBUG)

# command line parsing
parser = argparse.ArgumentParser(
        description='''download photos from instagram'''
)
parser.add_argument(
        '-i',
        '--id', 
        help='''id of the user to download the albums of
        For instance if you see a url like this:
            https://www.instagram.com/ada_123456789
        then the id for this script will be:
            ada_123456789
        '''
)
parser.add_argument(
        '-d',
        '--debug',
        help='debug requests',
        default=False,
        action='store_true',
)
args = parser.parse_args()
if args.id is None:
    parser.error('-i/--id must be given')
if args.debug:
    debug_requests()

url='https://www.instagram.com/{id}/'.format(id=args.id)
logger.debug('url is [%s]', url)
r = requests.get(url)
root = scrape.utils.get_real_content(r)

urls=[]
e_a = root.xpath('//script[re:match(text(), "^window._sharedData")]')
assert len(e_a)==1
e_a = e_a[0]
data = e_a.text
json_text = data[data.find('{'):data.rfind('}')+1]
#print(json_text)
d=json.loads(json_text)
l=d['entry_data']['ProfilePage']
assert(len(l)==1)
c=l[0]["user"]
urls.append(c['profile_pic_url_hd'])
user_id=c['id']
#json.dump(c, sys.stdout, indent=4)
list_node=c['media']['nodes']
for x in list_node:
    urls.append(x['display_src'])

scrape.utils.download_urls(urls)
