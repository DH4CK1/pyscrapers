#!/usr/bin/python3

'''
This script scrapes photos from instagram.com

For instance if you see a profile of a user like this:
    https://www.instagram.com/ada_123456789
then the id for this script will be:
    ada_123456789

How this works?
When you fetch the page of a user on instagram you get an html with a javascript embedded
in it with a json object embedded in that. This json object describes the user, his id,
his profile photo and the first 12 images for that user.
If you want to more photos you have to do a follow-up AJAX request to the server.

References:
- http://docs.python-requests.org/en/master
- http://docs.python-guide.org/en/latest/scenarios/scrape
'''

import requests # for post
import lxml.html # for fromstring
import lxml.etree # for tostring
import json # for loads
import shutil # for copyfileobj
import sys # for argv
import logging # for basicConfig, getLogger
import argparse  # for ArgumentParser
import browser_cookie3 # for firefox
import scrape.utils # for download_urls, get_real_content, debug_requests

# code

# register regular expressions with lxml
# this means that we can use regular expression functions like 'match'
# by specifying 're:match' in our xpath expressions
ns = lxml.etree.FunctionNamespace("http://exslt.org/regular-expressions")
ns.prefix = 're'

# set up the logger
logging.basicConfig()
logger=logging.getLogger(__name__)
logger.setLevel(logging.INFO)
#logger.setLevel(logging.DEBUG)

# command line parsing
parser = argparse.ArgumentParser(
        description='''download photos from instagram'''
)
parser.add_argument(
        '-i',
        '--id', 
        help='''id of the user to download the albums of
        For instance if you see a url like this:
            https://www.instagram.com/ada_123456789
        then the id for this script will be:
            ada_123456789
        '''
)
parser.add_argument(
        '-d',
        '--debug',
        help='debug requests',
        default=False,
        action='store_true',
)
args = parser.parse_args()
if args.id is None:
    parser.error('-i/--id must be given')
if args.debug:
    scrape.utils.debug_requests()

base='https://www.instagram.com'
url='{base}/{id}/'.format(base=base, id=args.id)
logger.debug('url is [%s]', url)
r = requests.get(url)
root = scrape.utils.get_real_content(r)
#scrape.utils.print_element(root)

urls=[]
e_a = root.xpath('//script[re:match(text(), "^window._sharedData")]')
assert len(e_a)==1
e_a = e_a[0]
data = e_a.text
json_text = data[data.find('{'):data.rfind('}')+1]
#print(json_text)
d=json.loads(json_text)
l=d['entry_data']['ProfilePage']
assert(len(l)==1)
c=l[0]["user"]
urls.append(c['profile_pic_url_hd'])
user_id=c['id']
#json.dump(c, sys.stdout, indent=4)
list_node=c['media']['nodes']
#for x in list_node:
#    urls.append(x['display_src'])

# now we need to do the follow up query
url2='{base}/query/'.format(base=base)
data={
    # the 5000 is the number of images you want (big number to get all)
    # the 0 in media.after is after what. 0 seems to return everything I think
    'q':'ig_user({0})'.format(user_id)+'{ media.after(0, 5000) { count, nodes { display_src } } }',
    'ref':'users::show',
    # this is constant as far as I can tell
    'query_id':'17842962958175392',
}
headers={
    # these two are neccessary or you wont get response from instagram
    'X-CSRFToken':r.cookies['csrftoken'],
    'Referer': url,
}
# you must send cookies and headers to get the data...
r2 = requests.post(url2, data=data, cookies=r.cookies, headers=headers)
root = scrape.utils.get_real_content(r2)
res=json.loads(root.text)
for node in res['media']['nodes']:
    urls.append(node['display_src'])
#scrape.utils.print_element(root)

scrape.utils.download_urls(urls)
